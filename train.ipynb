{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "TEXT8_URL = \"https://mattmahoney.net/dc/text8.zip\"\n",
    "\n",
    "def download_text8(out_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads the text8 dataset and saves it to the specified path.\n",
    "    \"\"\"\n",
    "    print(f\"Downloading text8 dataset to {out_path}\")\n",
    "    r = requests.get(TEXT8_URL)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def extract_text8(zip_path: str, out_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Extracts the text8 dataset from the specified zip file to the specified path.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text8 dataset from {zip_path} to {out_path}\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(out_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def get_text8(out_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads and extracts the text8 dataset to the specified path.\n",
    "    The returned path is the path to the extracted text8 file.\n",
    "    \"\"\"\n",
    "    zip_path = out_path + \".zip\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        download_text8(zip_path)\n",
    "    if not os.path.exists(out_path):\n",
    "        extract_text8(zip_path, out_path)\n",
    "    return os.path.join(out_path, \"text8\")\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "path = get_text8(\"data/text8\")\n",
    "print(path)\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from tok import CharacterTokenizer\n",
    "\n",
    "vocab = string.ascii_lowercase + \" \"\n",
    "tok = CharacterTokenizer(vocab, model_max_length=256)\n",
    "# check that encoding then decoding is the identity function\n",
    "a = tok.decode(tok.encode(text[:100]))\n",
    "b = text[:100]\n",
    "assert a == b, f\"Encoding then decoding is not the identity function! {a} != {b}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the entire text\n",
    "tokens = tok.encode(text, return_tensors=\"pt\")\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import random\n",
    "from jaxtyping import Int\n",
    "from typing import Generator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def chunk_dataset(tokens: Int[Tensor, \"1 D\"], chunk_size: int, rng: random.Random) -> Int[Tensor, \"n_chunks D\"]:\n",
    "    \"\"\"\n",
    "    Chunks the dataset into sequences of length chunk_size, randomly shuffled.\n",
    "    \"\"\"\n",
    "    chunked = list(tokens.chunk(tokens.shape[-1] // chunk_size, dim=-1))\n",
    "    rng.shuffle(chunked)\n",
    "    return torch.cat(chunked, dim=0)\n",
    "\n",
    "class Text8Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokens: Int[Tensor, \"1 D\"], chunk_size: int, rng: random.Random):\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.chunk_size = chunk_size\n",
    "        self.rng = rng\n",
    "        self.tokens_chunked = chunk_dataset(self.tokens, chunk_size, rng)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tokens.shape[-1] // self.chunk_size\n",
    "\n",
    "    def __getitem__(self, index: int) -> Int[Tensor, \"1 D\"]:\n",
    "        return self.tokens_chunked[index]\n",
    "\n",
    "def get_optim_groups(model: torch.nn.Module, weight_decay: float):\n",
    "    # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "    decay = set()\n",
    "    no_decay = set()\n",
    "    whitelist_weight_modules = (torch.nn.Linear, )\n",
    "    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "            # random note: because named_modules and named_parameters are recursive\n",
    "            # we will see the same tensors p many many times. but doing it this way\n",
    "            # allows us to know which parent module any tensor p belongs to...\n",
    "            if pn.endswith('bias'):\n",
    "                # all biases will not be decayed\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                # weights of whitelist modules will be weight decayed\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                # weights of blacklist modules will NOT be weight decayed\n",
    "                no_decay.add(fpn)\n",
    "\n",
    "    # validate that we considered every parameter\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    inter_params = decay & no_decay\n",
    "    union_params = decay | no_decay\n",
    "    assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "    assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "    # create the pytorch optimizer object\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optim_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login <KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from bfn import get_gpt_net, BayesianFlowNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 1e-1\n",
    "    BATCH_SIZE = 16\n",
    "    SEQ_LEN = D = 256\n",
    "    HIDDEN_DIM = 768\n",
    "    NUM_LAYERS = 24\n",
    "    NUM_HEADS = 12\n",
    "    DROPOUT = 0.0\n",
    "    SEED = 0\n",
    "    ADAM_BETAS = (0.9, 0.98)\n",
    "    N_BATCHES = 10000\n",
    "    VOCAB_SIZE = tok.vocab_size\n",
    "    LOSS_EMA = 0.99\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "wandb.init(project=\"bfn\", entity=\"conjecture_engineering\", config=CONFIG.__dict__)\n",
    "\n",
    "rng = random.Random(CONFIG.SEED)\n",
    "torch_rng = torch.Generator()\n",
    "torch_rng.manual_seed(CONFIG.SEED)\n",
    "dataset = Text8Dataset(tokens, chunk_size=CONFIG.SEQ_LEN, rng=rng)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    generator=torch_rng,\n",
    ")\n",
    "dataloader_iter = iter(dataloader)\n",
    "print(f\"Dataset has {len(dataset)} chunks of size {CONFIG.SEQ_LEN}.\")\n",
    "print(\"loading model...\")\n",
    "gpt = get_gpt_net(D=CONFIG.D + 1, vocab_size=CONFIG.VOCAB_SIZE, hidden_dim=CONFIG.HIDDEN_DIM, num_layers=CONFIG.NUM_LAYERS, num_heads=CONFIG.NUM_HEADS, dropout=CONFIG.DROPOUT) # D+1 because timestep is added to the last position in the sequence\n",
    "print('gpt loaded')\n",
    "bfn = BayesianFlowNetwork(gpt, D=CONFIG.D, vocab_size=CONFIG.VOCAB_SIZE, beta=3.0)\n",
    "print('bfn loaded')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "bfn = bfn.to(device)\n",
    "param_groups = get_optim_groups(bfn, weight_decay=CONFIG.WEIGHT_DECAY)\n",
    "optim = AdamW(param_groups, lr=CONFIG.LR, betas=CONFIG.ADAM_BETAS)\n",
    "\n",
    "losses = []\n",
    "print(\"training...\")\n",
    "pbar = tqdm(range(CONFIG.N_BATCHES))\n",
    "for i in pbar:\n",
    "    optim.zero_grad()\n",
    "\n",
    "    X = next(dataloader_iter).to(device)\n",
    "    loss = bfn.process(X)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    wandb.log({\"train/loss\": loss.item()})\n",
    "    losses.append(loss.item())\n",
    "    if i % 1000 == 0:\n",
    "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "# smooth losses by exponential moving average\n",
    "losses_ema = []\n",
    "ema = losses[0]\n",
    "for loss in losses:\n",
    "    ema = CONFIG.LOSS_EMA * ema + (1 - CONFIG.LOSS_EMA) * loss\n",
    "    losses_ema.append(ema)\n",
    "\n",
    "plt.plot(losses_ema)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out model\n",
    "torch.save(bfn.net.state_dict(), \"bfn_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample:\n",
    "bfn = bfn.eval()\n",
    "tokens = bfn.sample(batch_size=1, nb_steps=100, device=device)\n",
    "print(tok.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
